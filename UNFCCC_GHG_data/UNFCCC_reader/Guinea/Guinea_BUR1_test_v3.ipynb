{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461e34a0-47b1-44a7-ba1a-77db66ea783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set environment variable (only for jupyter notebook)\n",
    "import os\n",
    "os.environ[\"UNFCCC_GHG_ROOT_PATH\"] = \"/Users/danielbusch/Documents/UNFCCC_non-AnnexI_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dd87db-4956-4bb1-937a-84629bfce95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "import primap2 as pm2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from UNFCCC_GHG_data.helper import downloaded_data_path, extracted_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37d6d49-076c-4823-a486-83fbda3fa33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###\n",
    "# configuration\n",
    "# ###\n",
    "\n",
    "input_folder = downloaded_data_path / 'UNFCCC' / 'Guinea' / 'BUR1'\n",
    "output_folder = extracted_data_path / 'UNFCCC' / 'Guinea'\n",
    "if not output_folder.exists():\n",
    "    output_folder.mkdir()\n",
    "\n",
    "pdf_file = \"Rapport_IGES-Guinee-BUR1_VF.pdf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bf46ce-441e-4247-b62a-ce5ebcf26cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primap2 format conversion\n",
    "coords_cols = {\n",
    "    \"category\": \"category\",\n",
    "    \"entity\": \"entity\",\n",
    "    \"unit\": \"unit\",\n",
    "}\n",
    "\n",
    "coords_defaults = {\n",
    "    \"source\": \"GIN-GHG-Inventory\",\n",
    "    \"provenance\": \"measured\",\n",
    "    \"area\": \"GIN\",\n",
    "    \"scenario\": \"BUR1\",\n",
    "}\n",
    "\n",
    "coords_terminologies = {\n",
    "    \"area\": \"ISO3\",\n",
    "    # TODO check if this is correct\n",
    "    \"category\": \"IPCC1996_2006_GIN_Inv\",\n",
    "    \"scenario\": \"PRIMAP\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23676d59-d7e9-455c-b713-7ce98b92d5d7",
   "metadata": {},
   "source": [
    "### Q: How to choose gwp_to_use?\n",
    "### Q: 'unit' and 'category' are 'PRIMAP1'. Are there other options?\n",
    "### Q: Why are we mapping 'NMVOCs': 'NMVOC', wouldn't it be easier to name it NMVOC in the first place?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953ddab6-07ee-4b60-82f0-f2e9ca76b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are we choosing this gwp\n",
    "# TODO: Check in report what they use, page 20\n",
    "# there would be an error later if there are inconsistencies\n",
    "# global warming potentials nachlesen und dann per hand aufsummieren (geht hier nicht)\n",
    "# TODO: check what gwp conversion there are\n",
    "gwp_to_use = \"AR4GWP100\"\n",
    "coords_value_mapping = {\n",
    "    'main' : {\n",
    "        # automatische mappings, PRIMAP1 sind ohne die Punkte\n",
    "        \"unit\": \"PRIMAP1\", # keine Zeiteinheiten, andere Option ist ein dict wie bei entity\n",
    "        \"category\": \"PRIMAP1\",\n",
    "        \"entity\": {\n",
    "            'HFCs': f\"HFCS ({gwp_to_use})\",\n",
    "            'PFCs': f\"PFCS ({gwp_to_use})\",\n",
    "            'SF6' : f\"SF6 ({gwp_to_use})\",\n",
    "            'NMVOCs': 'NMVOC',\n",
    "            }\n",
    "    },\n",
    "    'energy' : {\n",
    "    \"unit\": \"PRIMAP1\",\n",
    "    \"category\": \"PRIMAP1\",\n",
    "    \"entity\": {\n",
    "        'NMVOCs': 'NMVOC',\n",
    "    }\n",
    "    },\n",
    "    'lulucf' : {\n",
    "    \"unit\": \"PRIMAP1\",\n",
    "    \"category\": \"PRIMAP1\",\n",
    "    \"entity\": {\n",
    "        'NMVOCs': 'NMVOC',\n",
    "    }\n",
    "    },\n",
    "    'waste' : {\n",
    "    \"unit\": \"PRIMAP1\",\n",
    "    \"category\": \"PRIMAP1\",\n",
    "    \"entity\": {\n",
    "        'NMVOCs': 'NMVOC',\n",
    "    }\n",
    "    },\n",
    "    'trend' : {\n",
    "    \"unit\": \"PRIMAP1\",\n",
    "    \"category\": \"PRIMAP1\",\n",
    "    },\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "filter_remove = {\n",
    "    'f_memo': {\"category\": \"MEMO\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef888811-5803-4df7-8fd8-06830e6d9bce",
   "metadata": {},
   "source": [
    "### Q: What to put under references and rights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b39c1a-700c-46f9-a3f5-33549658ad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = {\n",
    "    \"references\": \"placeholder\", # link UNFCCC Seit\n",
    "    \"rights\": \"\", # leer passt\n",
    "    \"contact\": \"mail@johannes-guetschow.de\", # my e-mail\n",
    "    \"title\": \"Guinea. Biennial update report (BUR). BUR1\",\n",
    "    \"comment\": \"Read fom pdf by Daniel Busch\",\n",
    "    \"institution\": \"UNFCCC\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2390fb91-d976-47f9-9236-a6c838e1fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_def_templates = {\n",
    "    '110': {\n",
    "        \"area\": ['36,718,589,87'],\n",
    "        \"cols\": ['290,340,368,392,425,445,465,497,535,564'],\n",
    "    },\n",
    "    '111': {\n",
    "        \"area\": ['36,736,587,107'],\n",
    "        \"cols\": ['293,335,369,399,424,445,468,497,535,565'],\n",
    "    },\n",
    "    '112': {\n",
    "        \"area\": ['35,733,588,106'],\n",
    "        \"cols\": ['293,335,369,399,424,445,468,497,535,565'],\n",
    "    },\n",
    "    '113': {\n",
    "        \"area\": ['35,733,588,106'],\n",
    "        \"cols\": ['293,335,365,399,424,445,468,497,535,565'],\n",
    "    },\n",
    "    '131' : {\n",
    "                \"area\": ['36,718,590,83'],\n",
    "                \"cols\": ['293,332,370,406,442,480,516,554'],\n",
    "            },\n",
    "}\n",
    "\n",
    "# for main table\n",
    "header_inventory = ['Greenhouse gas source and sink categories',\n",
    "                   'CO2', 'CH4', \"N2O\", 'HFCs', 'PFCs', 'SF6', 'NOx', 'CO', 'NMVOCs','SO2'\n",
    "                   ]\n",
    "# TODO the extra '-' may be wrong here, check again!\n",
    "unit_inventory = ['-'] + ['Gg'] * len(header_inventory) # one extra for the category columns\n",
    "unit_inventory[4] = \"GgCO2eq\"\n",
    "unit_inventory[5] = \"GgCO2eq\"\n",
    "unit_inventory[6] = \"GgCO2eq\"\n",
    "\n",
    "# for energy tables\n",
    "# use header as in tables, when possible. If it's a lot easier change it \n",
    "header_energy = ['Greenhouse gas source and sink categories',\n",
    "                   'CO2', 'CH4', \"N2O\", 'NOx', 'CO', 'NMVOCs','SO2'\n",
    "                   ]\n",
    "unit_energy = ['-'] + ['Gg'] * len(header_energy) # one extra for the category columns\n",
    "\n",
    "# for lulucf tables\n",
    "header_lulucf = ['Greenhouse gas source and sink categories', 'CO2', 'CH4', \"N2O\", 'NOx', 'CO', 'NMVOCs']\n",
    "unit_lulucf = ['-'] + ['Gg'] * (len(header_lulucf) - 1)\n",
    "\n",
    "# for waste table\n",
    "header_waste = ['Greenhouse gas source and sink categories', 'CO2', 'CH4', \"N2O\", 'NOx', 'CO', 'NMVOCs', 'SO2']\n",
    "unit_waste = ['-'] + ['Gg'] * (len(header_waste) - 1)\n",
    "\n",
    "# for trend table (unit is always Gg for this table)\n",
    "header_trend = ['data1990', 'data1995', \"data2000\", 'data2005', 'data2010', 'data2015', 'data2018', 'data2019']\n",
    "\n",
    "\n",
    "# define config dict\n",
    "inv_conf = {\n",
    "    'header': header_inventory,\n",
    "    'unit': unit_inventory,\n",
    "    'header_energy' : header_energy,\n",
    "    'unit_energy' : unit_energy,\n",
    "    'header_lulucf' : header_lulucf,\n",
    "    'unit_lulucf' : unit_lulucf,\n",
    "    'header_waste' : header_waste,\n",
    "    'unit_waste' : unit_waste,\n",
    "    'header_trend' : header_trend,\n",
    "    'entity_row': 0,\n",
    "    'unit_row': 1,\n",
    "    'index_cols': \"Greenhouse gas source and sink categories\",\n",
    "    'year': {'110' : 1990,\n",
    "             '111' : 2000,\n",
    "             '112' : 2010,\n",
    "             '113' : 2019,\n",
    "             '116' : 1990,\n",
    "             '117' : 2000,\n",
    "             '118' : 2010,\n",
    "             '119' : 2019,\n",
    "             '124' : 1990,\n",
    "             '125' : 2000,\n",
    "             '126' : 2010,\n",
    "             '127' : 2019,\n",
    "            },\n",
    "    'header_long': [\"orig_cat_name\", \"entity\", \"unit\", \"time\", \"data\"],\n",
    "    \"cat_code_regexp\" : r'^(?P<code>[a-zA-Z0-9\\.]{1,11})[\\s\\.].*'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0b97f8-acbb-4df1-9764-b2d0f6af39ba",
   "metadata": {},
   "source": [
    "## 1. Read main tables - pages 110, 111, 112, 113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4357ddd0-e9ee-4b2b-a765-c36411df63e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = ['110', '111', '112', '113']\n",
    "df_all_dict = {}\n",
    "for page in pages:\n",
    "    \n",
    "    print(\"-\"*45)\n",
    "    print(f\"Reading table from page {page}.\")\n",
    "    \n",
    "    tables_inventory_original = camelot.read_pdf(\n",
    "        str(input_folder / pdf_file),\n",
    "        pages=page,\n",
    "        table_areas=page_def_templates[page][\"area\"],\n",
    "        columns=page_def_templates[page][\"cols\"],\n",
    "        flavor=\"stream\",\n",
    "        split_text=True)\n",
    "    \n",
    "    print(\"Reading complete.\")\n",
    "    \n",
    "    df_inventory = tables_inventory_original[0].df.copy()\n",
    "\n",
    "    # move broken text in correct row (page 113 is fine)\n",
    "    if page in ['110', '111', '112']:\n",
    "        df_inventory.at[4, 0] = \"1.A.1 - Industries énergétiques\"\n",
    "        df_inventory = df_inventory.drop(index=3)\n",
    "        df_inventory.at[8, 0] = \"1.A.4 - Autres secteurs\"\n",
    "        df_inventory = df_inventory.drop(index=7)\n",
    "\n",
    "    # add header and unit\n",
    "    df_header = pd.DataFrame([inv_conf[\"header\"], inv_conf[\"unit\"]])\n",
    "    df_inventory = pd.concat([df_header, df_inventory], axis=0, join='outer').reset_index(drop=True)\n",
    "    df_inventory = pm2.pm2io.nir_add_unit_information(df_inventory,\n",
    "                                                  unit_row=inv_conf[\"unit_row\"],\n",
    "                                                  entity_row=inv_conf[\"entity_row\"],\n",
    "                                                  regexp_entity=\".*\",\n",
    "                                                  regexp_unit=\".*\",\n",
    "                                                  default_unit=\"Gg\")\n",
    "    \n",
    "    print(\"Added unit information.\")\n",
    "    \n",
    "    # set index\n",
    "    df_inventory = df_inventory.set_index(inv_conf[\"index_cols\"])\n",
    "\n",
    "    # convert to long format\n",
    "    df_inventory_long = pm2.pm2io.nir_convert_df_to_long(df_inventory, inv_conf[\"year\"][page],\n",
    "                                                     inv_conf[\"header_long\"])\n",
    "\n",
    "    # extract category from tuple\n",
    "    df_inventory_long[\"orig_cat_name\"] = df_inventory_long[\"orig_cat_name\"].str[0] \n",
    "\n",
    "    # prep for conversion to PM2 IF and native format\n",
    "    # make a copy of the categories row\n",
    "    df_inventory_long[\"category\"] = df_inventory_long[\"orig_cat_name\"]\n",
    "\n",
    "    # replace cat names by codes in col \"category\"\n",
    "    # first the manual replacements\n",
    "    # TODO: move this to config section\n",
    "    inv_conf[\"cat_codes_manual\"]['main'] = {\n",
    "            'Éléments pour mémoire': 'MEMO',\n",
    "            'Soutes internationales': 'M.BK',\n",
    "            '1.A.3.a.i - Aviation internationale (soutes internationales)': 'M.BK.A',\n",
    "            '1.A.3.d.i - Navigation internationale (soutes internationales)' : 'M.BK.M',\n",
    "            '1.A.5.c - Opérations multilatérales' : 'M.MULTIOP',\n",
    "            'Total des émissions et absorptions nationales': \"0\",\n",
    "            '2A5: Autre': '2A5', \n",
    "        }\n",
    "    df_inventory_long[\"category\"] = \\\n",
    "        df_inventory_long[\"category\"].replace(inv_conf[\"cat_codes_manual\"]['main'])  \n",
    "\n",
    "    df_inventory_long[\"category\"] = df_inventory_long[\"category\"].str.replace(\".\", \"\")\n",
    "    \n",
    "    # then the regex replacements\n",
    "    repl = lambda m: m.group('code')\n",
    "    df_inventory_long[\"category\"] = \\\n",
    "        df_inventory_long[\"category\"].str.replace(inv_conf[\"cat_code_regexp\"], repl,\n",
    "                                              regex=True)\n",
    "\n",
    "    df_inventory_long = df_inventory_long.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    \n",
    "    df_inventory_long[\"data\"] = df_inventory_long[\"data\"].str.replace(\",\", \".\")\n",
    "    df_inventory_long[\"data\"] = df_inventory_long[\"data\"].str.replace(\"NE1\", \"NE\")\n",
    "\n",
    "    # make sure all col headers are str\n",
    "    df_inventory_long.columns = df_inventory_long.columns.map(str)\n",
    "    df_inventory_long = df_inventory_long.drop(columns=[\"orig_cat_name\"])\n",
    "    \n",
    "    df_all_dict[page] = df_inventory_long\n",
    "\n",
    "df_all = pd.concat([df_all_dict['110'], df_all_dict['111'], df_all_dict['112'], df_all_dict['113']],\n",
    "                      axis=0,\n",
    "                      join='outer').reset_index(drop=True)\n",
    "\n",
    "print(\"Converting to interchange format.\")\n",
    "df_all_IF = pm2.pm2io.convert_long_dataframe_if(\n",
    "    df_all,\n",
    "    coords_cols=coords_cols,\n",
    "    #add_coords_cols=add_coords_cols,\n",
    "    coords_defaults=coords_defaults,\n",
    "    coords_terminologies=coords_terminologies,\n",
    "    coords_value_mapping=coords_value_mapping['main'],\n",
    "    #coords_value_filling=coords_value_filling,\n",
    "    filter_remove=filter_remove,\n",
    "    #filter_keep=filter_keep,\n",
    "    meta_data=meta_data,\n",
    "    convert_str=True,\n",
    "    time_format=\"%Y\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a4535e-3abc-45d0-9309-fd7991b1cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test individual values from the tables ###\n",
    "# TODO and note: this function is work in progress\n",
    "# Use assert statements and print error message\n",
    "# with category, entity, year, expected value and actual value\n",
    "\n",
    "### Test individual values from the tables ###\n",
    "def assert_individual_value(\n",
    "    df,\n",
    "    category_column,\n",
    "    entity_column,\n",
    "    category,\n",
    "    entity,\n",
    "    year,\n",
    "    expected_value\n",
    "):\n",
    "    arr = df.loc[(df[category_column] == category) & (df[entity_column] == entity), year].values\n",
    "    print(arr)\n",
    "    if len(arr) > 1:\n",
    "        print(f\"More than one value found for {category}, {entity}, {year}!\")\n",
    "\n",
    "    # TODO: It looks like this will be true when the value equals 0\n",
    "    if not arr:\n",
    "        print((f\"No value found for {category}, {entity}, {year}!\"))\n",
    "            \n",
    "    if not arr[0] == expected_value:\n",
    "        print(f\"Expected value {expected_value}, actual value is {arr[0]}\")\n",
    "\n",
    "    if arr[0] == expected_value:\n",
    "        print(\"Value matches expected value.\")\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "test_cases = {\n",
    "    \"1\" : {\n",
    "        \"category\" : \"1.A.1\",\n",
    "        'entity' : \"CO2\",\n",
    "        \"year\" : \"2010\",\n",
    "        \"expected_value\" : 422.474,\n",
    "    },\n",
    "    \"2\" : {\n",
    "        \"category\" : \"2\",\n",
    "        'entity' : \"SO2\",\n",
    "        \"year\" : \"1990\",\n",
    "        \"expected_value\" : 0.097,\n",
    "    },\n",
    "    \"3\" : {\n",
    "        \"category\" : \"1.A.3.a.i\",\n",
    "        'entity' : \"N2O\",\n",
    "        \"year\" : \"2000\",\n",
    "        \"expected_value\" : 6e-5,\n",
    "    },\n",
    "    '4' : {\n",
    "        \"category\" : \"2.H.2\",\n",
    "        'entity' : \"NMVOC\",\n",
    "        \"year\" : \"2019\",\n",
    "        \"expected_value\" : 2.506,\n",
    "    },\n",
    "    '5' : {\n",
    "        \"category\" : \"1.A.1\",\n",
    "        'entity' : \"CH4\",\n",
    "        \"year\" : \"2019\",\n",
    "        \"expected_value\" : 0.0011,\n",
    "    }\n",
    "}\n",
    "\n",
    "for key in test_cases.keys():\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Testing combination {test_cases[key][\"category\"]}, {test_cases[key][\"entity\"]}, {test_cases[key][\"year\"]}.\")\n",
    "    assert_individual_value(\n",
    "                    df = df_all_IF,\n",
    "                    category_column = \"category (IPCC1996_2006_GIN_Inv)\",\n",
    "                    entity_column = \"entity\",\n",
    "                    category = test_cases[key][\"category\"],\n",
    "                    entity = test_cases[key][\"entity\"],\n",
    "                    year = test_cases[key][\"year\"],\n",
    "                    expected_value = test_cases[key][\"expected_value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23258414-84b2-4a99-8f48-f471f5ebf75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### check data for errors ###\n",
    "# print a few things to see if it looks \"normal\"\n",
    "for c in df_all_IF.columns:\n",
    "    print('-'*50)\n",
    "    print(f\"Unique values in column {c}\")\n",
    "    print(df_all_IF[c].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07812254-fb73-4cb5-ae45-a96a2f2273d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### convert to primap2 format ###\n",
    "data_pm2_main = pm2.pm2io.from_interchange_format(df_all_IF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d4e68e-f1f4-4c7d-b710-c749296a16ca",
   "metadata": {},
   "source": [
    "## 2. Read in sector tables for energy - pages 116, 117, 118, 119"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251c3495-8506-4f43-9a97-094b5fb16947",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = ['116', '117', '118', '119']\n",
    "df_energy_dict = {}\n",
    "for page in pages:\n",
    "    print(\"-\"*45)\n",
    "    print(f\"Reading table from page {page}.\")\n",
    "    \n",
    "    tables_inventory_original = camelot.read_pdf(\n",
    "        str(input_folder / pdf_file),\n",
    "        pages=page,\n",
    "        flavor=\"lattice\",\n",
    "        split_text=True\n",
    "        )\n",
    "    \n",
    "    print(\"Reading complete.\")\n",
    "\n",
    "    # cut last two lines of second table to ignore additional information regarding biomass for energy production \n",
    "    df_energy_year = pd.concat([tables_inventory_original[0].df[2:],\n",
    "                                tables_inventory_original[1].df[3:-2]],\n",
    "                                axis=0,\n",
    "                                join='outer').reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    # drop duplicate lines - 1.A.3.d.i / 1.A.3.a.i / 1.A.5.c\n",
    "    # TODO: better to find the index of the line and then drop it by the index\n",
    "    df_energy_year = df_energy_year.drop(index=[27, 32, 50])  \n",
    "    \n",
    "    # add header and unit\n",
    "    df_header = pd.DataFrame([inv_conf[\"header_energy\"], inv_conf[\"unit_energy\"]])\n",
    "\n",
    "    df_energy_year = pd.concat([df_header, df_energy_year], axis=0, join='outer').reset_index(drop=True)\n",
    "    \n",
    "    df_energy_year = pm2.pm2io.nir_add_unit_information(df_energy_year,\n",
    "                                                  unit_row=inv_conf[\"unit_row\"],\n",
    "                                                  entity_row=inv_conf[\"entity_row\"],\n",
    "                                                  regexp_entity=\".*\",\n",
    "                                                  regexp_unit=\".*\",\n",
    "                                                  default_unit=\"Gg\")\n",
    "    \n",
    "    print(\"Added unit information.\")\n",
    "    # set index\n",
    "    df_energy_year = df_energy_year.set_index(inv_conf[\"index_cols\"])\n",
    "\n",
    "    # convert to long format\n",
    "    df_energy_year_long = pm2.pm2io.nir_convert_df_to_long(df_energy_year, inv_conf[\"year\"][page],\n",
    "                                                     inv_conf[\"header_long\"])\n",
    "    \n",
    "    # extract from tuple\n",
    "    df_energy_year_long[\"orig_cat_name\"] = df_energy_year_long[\"orig_cat_name\"].str[0] \n",
    "\n",
    "    # prep for conversion to PM2 IF and native format\n",
    "    # make a copy of the categories row\n",
    "    df_energy_year_long[\"category\"] = df_energy_year_long[\"orig_cat_name\"]\n",
    "\n",
    "    # replace individual categories\n",
    "    # TODO: move to config section\n",
    "    inv_conf[\"cat_codes_manual\"]['energy'] = {\n",
    "            'International Bunkers': 'MEMO',\n",
    "            '1.A.3.a.i - Aviation internationale (soutes internationales)': 'M.BK.A',\n",
    "            '1.A.3.d.i - Navigation internationale (soutes internationales)' : 'M.BK.M',\n",
    "            '1.A.5.c - Opérations multilatérales' : 'M.MULTIOP',\n",
    "        }\n",
    "\n",
    "    # replace cat names by codes in col \"category\"\n",
    "    # first the manual replacements\n",
    "    df_energy_year_long[\"category\"] = df_energy_year_long[\"category\"].str.replace('\\n' ,'')\n",
    "    df_energy_year_long[\"category\"] = \\\n",
    "        df_energy_year_long[\"category\"].replace(inv_conf[\"cat_codes_manual\"]['energy'])\n",
    "\n",
    "    df_energy_year_long[\"category\"] = df_energy_year_long[\"category\"].str.replace(\".\", \"\")\n",
    "    \n",
    "    inv_conf[\"cat_code_regexp\"] = r'^(?P<code>[a-zA-Z0-9\\.]{1,11})[\\s\\.].*'\n",
    "\n",
    "    # then the regex replacements\n",
    "    repl = lambda m: m.group('code')\n",
    "    df_energy_year_long[\"category\"] = \\\n",
    "        df_energy_year_long[\"category\"].str.replace(inv_conf[\"cat_code_regexp\"], repl,\n",
    "                                              regex=True)\n",
    "\n",
    "    df_energy_year_long = df_energy_year_long.reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    df_energy_year_long[\"data\"] = df_energy_year_long[\"data\"].str.replace(\",\", \".\")\n",
    "    df_energy_year_long[\"data\"] = df_energy_year_long[\"data\"].str.replace(\"NE1\", \"NE\")\n",
    "\n",
    "    # make sure all col headers are str\n",
    "    df_energy_year_long.columns = df_energy_year_long.columns.map(str)\n",
    "    df_energy_year_long = df_energy_year_long.drop(columns=[\"orig_cat_name\"])\n",
    "    \n",
    "    df_energy_dict[page] = df_energy_year_long\n",
    "\n",
    "df_energy = pd.concat([df_energy_dict['116'], df_energy_dict['117'], df_energy_dict['118'], df_energy_dict['119']],\n",
    "                      axis=0,\n",
    "                      join='outer').reset_index(drop=True)\n",
    "\n",
    "print(\"Converting to interchange format.\")\n",
    "df_energy_IF = pm2.pm2io.convert_long_dataframe_if(\n",
    "    df_energy,\n",
    "    coords_cols=coords_cols,\n",
    "    #add_coords_cols=add_coords_cols,\n",
    "    coords_defaults=coords_defaults,\n",
    "    coords_terminologies=coords_terminologies,\n",
    "    coords_value_mapping=coords_value_mapping['energy'],\n",
    "    #coords_value_filling=coords_value_filling,\n",
    "    filter_remove=filter_remove,\n",
    "    #filter_keep=filter_keep,\n",
    "    meta_data=meta_data,\n",
    "    convert_str=True,\n",
    "    time_format=\"%Y\",\n",
    "    )\n",
    "    \n",
    "df_energy_IF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa29dc-f62b-4010-bfed-8cd588675475",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = {\n",
    "    \"1\" : {\n",
    "        \"category\" : \"1.A.2.k\",\n",
    "        'entity' : \"CH4\",\n",
    "        \"year\" : \"1990\",\n",
    "        \"expected_value\" : 3e-05,\n",
    "    },\n",
    "    \"2\" : {\n",
    "        \"category\" : \"1.A.4.c.i\",\n",
    "        'entity' : \"CO\",\n",
    "        \"year\" : \"1990\",\n",
    "        \"expected_value\" : 0.0016,\n",
    "    },\n",
    "    \"3\" : {\n",
    "        \"category\" : \"1.A.3.a.i\",\n",
    "        'entity' : \"NMVOC\",\n",
    "        \"year\" : \"2000\",\n",
    "        \"expected_value\" : 0.0002,\n",
    "    },\n",
    "    '4' : {\n",
    "        \"category\" : \"1\",\n",
    "        'entity' : \"SO2\",\n",
    "        \"year\" : \"2010\",\n",
    "        \"expected_value\" : 0,\n",
    "    },\n",
    "    '5' : {\n",
    "        \"category\" : \"1.A.2.k\",\n",
    "        'entity' : \"N2O\",\n",
    "        \"year\" : \"2019\",\n",
    "        \"expected_value\" : 7e-06,\n",
    "    }\n",
    "}\n",
    "\n",
    "for key in test_cases.keys():\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Testing combination {test_cases[key][\"category\"]}, {test_cases[key][\"entity\"]}, {test_cases[key][\"year\"]}.\")\n",
    "    assert_individual_value(\n",
    "                    df = df_energy_IF,\n",
    "                    category_column = \"category (IPCC1996_2006_GIN_Inv)\",\n",
    "                    entity_column = \"entity\",\n",
    "                    category = test_cases[key][\"category\"],\n",
    "                    entity = test_cases[key][\"entity\"],\n",
    "                    year = test_cases[key][\"year\"],\n",
    "                    expected_value = test_cases[key][\"expected_value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf727f7-3474-4f2e-9bcb-ebdd140a14c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### convert to primap2 format ###\n",
    "data_pm2_energy = pm2.pm2io.from_interchange_format(df_energy_IF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d557a318-ea3f-44ec-9187-c05da423fbca",
   "metadata": {},
   "source": [
    "# 3. Read in LULUCF table - pages 124, 125, 126, 127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d117f0-6bfc-468f-b9f2-f66d5eaf8f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = ['124', '125', '126', '127']\n",
    "df_lulucf_dict = {}\n",
    "for page in pages:\n",
    "    print(\"-\"*45)\n",
    "    print(f\"Reading table from page {page}.\")\n",
    "    \n",
    "    tables_inventory_original = camelot.read_pdf(\n",
    "    str(input_folder / pdf_file),\n",
    "    pages=page,\n",
    "    flavor=\"lattice\",\n",
    "    split_text=True\n",
    "    )\n",
    "    print(\"Reading complete.\")\n",
    "\n",
    "    if page == '127':\n",
    "        # table on page 127 has one extra row at the top\n",
    "        # and one extra category 3.A.1.j\n",
    "        df_lulucf_year = tables_inventory_original[0].df[3:]\n",
    "        # rename duplicate categories in tables\n",
    "        # TODO move to config section\n",
    "        replace_categories = [(19, \"3.A.2.a.i - Vaches laitières\"),\n",
    "                              (20, \"3.A.2.a.ii - Autres bovins\"),\n",
    "                              (21, \"3.A.2.b - Buffle\"),\n",
    "                              (22, \"3.A.2.c - Ovins\"),\n",
    "                              (23, \"3.A.2.d - Caprins\"),\n",
    "                              (24, \"3.A.2.e - Chameaux\"),\n",
    "                              (25, \"3.A.2.f - Chevaux\"),\n",
    "                              (26, \"3.A.2.g - Mules et ânes\"),\n",
    "                              (27, \"3.A.2.h - Porcins\"),\n",
    "                              (28, \"3.A.2.i - Volailles\"),\n",
    "                              (29, \"3.A.2.j - Autres (préciser)\"),]\n",
    "        for index, category_name in  replace_categories:\n",
    "            df_lulucf_year.at[index, 0] = category_name\n",
    "    else:\n",
    "        # cut first two lines\n",
    "        df_lulucf_year = tables_inventory_original[0].df[2:] \n",
    "\n",
    "        # TODO move to config section\n",
    "        replace_categories = [(17, \"3.A.2.a.i - Vaches laitières\"),\n",
    "                              (18, \"3.A.2.a.ii - Autres bovins\"),\n",
    "                              (19, \"3.A.2.b - Buffle\"),\n",
    "                              (20, \"3.A.2.c - Ovins\"),\n",
    "                              (21, \"3.A.2.d - Caprins\"),\n",
    "                              (22, \"3.A.2.e - Chameaux\"),\n",
    "                              (23, \"3.A.2.f - Chevaux\"),\n",
    "                              (24, \"3.A.2.g - Mules et ânes\"),\n",
    "                              (25, \"3.A.2.h - Porcins\"),\n",
    "                              (26, \"3.A.2.i - Volailles\"),]\n",
    "        for index, category_name in  replace_categories:\n",
    "            df_lulucf_year.at[index, 0] = category_name\n",
    "    \n",
    "    # add header and unit\n",
    "    df_header = pd.DataFrame([inv_conf[\"header_lulucf\"], inv_conf[\"unit_lulucf\"]])\n",
    "\n",
    "    df_lulucf_year = pd.concat([df_header, df_lulucf_year], axis=0, join='outer').reset_index(drop=True)\n",
    "\n",
    "    df_lulucf_year = pm2.pm2io.nir_add_unit_information(df_lulucf_year,\n",
    "                                                  unit_row=inv_conf[\"unit_row\"],\n",
    "                                                  entity_row=inv_conf[\"entity_row\"],\n",
    "                                                  regexp_entity=\".*\",\n",
    "                                                  regexp_unit=\".*\",\n",
    "                                                  default_unit=\"Gg\")\n",
    "\n",
    "    print(\"Added unit information.\")\n",
    "    \n",
    "    # set index\n",
    "    df_lulucf_year = df_lulucf_year.set_index(inv_conf[\"index_cols\"])\n",
    "\n",
    "    # convert to long format\n",
    "    df_lulucf_year_long = pm2.pm2io.nir_convert_df_to_long(df_lulucf_year, inv_conf[\"year\"][page],\n",
    "                                                     inv_conf[\"header_long\"])\n",
    "    \n",
    "    df_lulucf_year_long[\"orig_cat_name\"] = df_lulucf_year_long[\"orig_cat_name\"].str[0] # extract from tuple\n",
    "\n",
    "    # prep for conversion to PM2 IF and native format\n",
    "    # make a copy of the categories row\n",
    "    df_lulucf_year_long[\"category\"] = df_lulucf_year_long[\"orig_cat_name\"]\n",
    "   \n",
    "    # regex replacements\n",
    "    repl = lambda m: m.group('code')\n",
    "    df_lulucf_year_long[\"category\"] = \\\n",
    "        df_lulucf_year_long[\"category\"].str.replace(inv_conf[\"cat_code_regexp\"], repl,\n",
    "                                              regex=True)\n",
    "    \n",
    "    df_lulucf_year_long = df_lulucf_year_long.reset_index(drop=True)\n",
    "    \n",
    "    df_lulucf_year_long[\"data\"] = df_lulucf_year_long[\"data\"].str.replace(\",\", \".\")\n",
    "    df_lulucf_year_long[\"data\"] = df_lulucf_year_long[\"data\"].str.replace(\"NE1\", \"NE\")\n",
    "\n",
    "    # make sure all col headers are str\n",
    "    df_lulucf_year_long.columns = df_lulucf_year_long.columns.map(str)\n",
    "    df_lulucf_year_long = df_lulucf_year_long.drop(columns=[\"orig_cat_name\"])\n",
    "    \n",
    "    df_lulucf_dict[page] = df_lulucf_year_long\n",
    "\n",
    "df_lulucf = pd.concat([df_lulucf_dict['124'], df_lulucf_dict['125'], df_lulucf_dict['126'], df_lulucf_dict['127']],\n",
    "                      axis=0,\n",
    "                      join='outer').reset_index(drop=True)\n",
    "\n",
    "print(\"Converting to interchange format.\")\n",
    "df_lulucf_IF = pm2.pm2io.convert_long_dataframe_if(\n",
    "    df_lulucf,\n",
    "    coords_cols=coords_cols,\n",
    "    #add_coords_cols=add_coords_cols,\n",
    "    coords_defaults=coords_defaults,\n",
    "    coords_terminologies=coords_terminologies,\n",
    "    coords_value_mapping=coords_value_mapping['lulucf'],\n",
    "    #coords_value_filling=coords_value_filling,\n",
    "    filter_remove=filter_remove,\n",
    "    #filter_keep=filter_keep,\n",
    "    meta_data=meta_data,\n",
    "    convert_str=True,\n",
    "    time_format=\"%Y\",\n",
    "    )\n",
    "    \n",
    "df_lulucf_IF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d132ea2-655a-4363-9171-b81904a7d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### convert to primap2 format ###\n",
    "data_pm2_lulucf = pm2.pm2io.from_interchange_format(df_lulucf_IF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99c689e-1f26-42d5-8974-194373ce26f6",
   "metadata": {},
   "source": [
    "# 3. Read in Waste tables - pages 128, 130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf17dba-6af4-400f-9ec3-b5dd5b1b0a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are three tables for three years on page 128\n",
    "# and another tabel on page 130\n",
    "\n",
    "# read three tables\n",
    "page = '128'\n",
    "tables_inventory_original_128 = camelot.read_pdf(\n",
    "    str(input_folder / pdf_file),\n",
    "    pages=page,\n",
    "    flavor=\"lattice\",\n",
    "    split_text=True\n",
    ")\n",
    "\n",
    "# read last table\n",
    "page = '130'\n",
    "tables_inventory_original_130 = camelot.read_pdf(\n",
    "    str(input_folder / pdf_file),\n",
    "    pages=page,\n",
    "    flavor=\"lattice\",\n",
    "    split_text=True\n",
    ")\n",
    "\n",
    "# save to dict\n",
    "df_waste_years = {\n",
    "    '1990' : tables_inventory_original_128[0].df,\n",
    "    '2000' : tables_inventory_original_128[1].df,\n",
    "    '2010' : tables_inventory_original_128[2].df,\n",
    "    '2019' : tables_inventory_original_130[0].df,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0afb6e-db8b-41ae-b02d-e4a5d54ea5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_waste_dict = {}\n",
    "for year in df_waste_years.keys():\n",
    "    print(\"-\"*45)\n",
    "    print(f\"Processing table for {year}.\")\n",
    "\n",
    "    df_waste_year = df_waste_years[year][2:]\n",
    "    \n",
    "    # add header and unit\n",
    "    df_header = pd.DataFrame([inv_conf[\"header_waste\"], inv_conf[\"unit_waste\"]])\n",
    "\n",
    "    df_waste_year = pd.concat([df_header, df_waste_year], axis=0, join='outer').reset_index(drop=True)\n",
    "\n",
    "    df_waste_year = pm2.pm2io.nir_add_unit_information(df_waste_year,\n",
    "                                                  unit_row=inv_conf[\"unit_row\"],\n",
    "                                                  entity_row=inv_conf[\"entity_row\"],\n",
    "                                                  regexp_entity=\".*\",\n",
    "                                                  regexp_unit=\".*\",\n",
    "                                                  default_unit=\"Gg\")\n",
    "\n",
    "    print(\"Added unit information.\")\n",
    "    \n",
    "    # set index\n",
    "    df_waste_year = df_waste_year.set_index(inv_conf[\"index_cols\"])\n",
    "\n",
    "    # convert to long format\n",
    "    df_waste_year_long = pm2.pm2io.nir_convert_df_to_long(df_waste_year, year,\n",
    "                                                     inv_conf[\"header_long\"])\n",
    "    \n",
    "    df_waste_year_long[\"orig_cat_name\"] = df_waste_year_long[\"orig_cat_name\"].str[0]\n",
    "\n",
    "    # prep for conversion to PM2 IF and native format\n",
    "    # make a copy of the categories row\n",
    "    df_waste_year_long[\"category\"] = df_waste_year_long[\"orig_cat_name\"]\n",
    "\n",
    "    # regex replacements\n",
    "    repl = lambda m: m.group('code')\n",
    "    df_waste_year_long[\"category\"] = \\\n",
    "        df_waste_year_long[\"category\"].str.replace(inv_conf[\"cat_code_regexp\"], repl,\n",
    "                                              regex=True)\n",
    "    \n",
    "    df_waste_year_long = df_waste_year_long.reset_index(drop=True)\n",
    "\n",
    "    df_waste_year_long[\"category\"] = df_waste_year_long[\"category\"].str.replace(\".\", \"\")\n",
    "    df_waste_year_long[\"data\"] = df_waste_year_long[\"data\"].str.replace(\",\", \".\")\n",
    "    df_waste_year_long[\"data\"] = df_waste_year_long[\"data\"].str.replace(\"NE1\", \"NE\")\n",
    "\n",
    "    # make sure all col headers are str\n",
    "    df_waste_year_long.columns = df_waste_year_long.columns.map(str)\n",
    "    df_waste_year_long = df_waste_year_long.drop(columns=[\"orig_cat_name\"])\n",
    "    \n",
    "    df_waste_dict[year] = df_waste_year_long\n",
    "\n",
    "df_waste = pd.concat([df_waste_dict['1990'], df_waste_dict['2000'], df_waste_dict['2010'], df_waste_dict['2019']],\n",
    "                      axis=0,\n",
    "                      join='outer').reset_index(drop=True)\n",
    "\n",
    "print(\"Converting to interchange format.\")\n",
    "df_waste_IF = pm2.pm2io.convert_long_dataframe_if(\n",
    "    df_waste,\n",
    "    coords_cols=coords_cols,\n",
    "    #add_coords_cols=add_coords_cols,\n",
    "    coords_defaults=coords_defaults,\n",
    "    coords_terminologies=coords_terminologies,\n",
    "    coords_value_mapping=coords_value_mapping['waste'],\n",
    "    #coords_value_filling=coords_value_filling,\n",
    "    filter_remove=filter_remove,\n",
    "    #filter_keep=filter_keep,\n",
    "    meta_data=meta_data,\n",
    "    convert_str=True,\n",
    "    time_format=\"%Y\",\n",
    "    )\n",
    "    \n",
    "df_waste_IF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6628eacb-8a24-415b-a42e-04e929976f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "### convert to primap2 format ###\n",
    "data_pm2_waste = pm2.pm2io.from_interchange_format(df_waste_IF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba512153-1c65-4568-9bae-817fbf9cc9b3",
   "metadata": {},
   "source": [
    "# 4. Read in trend tables - pages 131 - 137"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e71c7b2-c301-4048-8b92-c9fc58a2501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib widget \n",
    "#camelot.plot(tables_inventory_original[0], kind='text')\n",
    "\n",
    "df_main_dict = {}\n",
    "pages = ['131', '132', '133', '134', '135', '136', '137']\n",
    "entities = ['CO2', 'CH4', 'N2O', 'NOx', 'CO', 'NMVOCs', 'SO2']\n",
    "\n",
    "# for this set of tables every page is a different entity\n",
    "for page, entity in zip(pages, entities):\n",
    "\n",
    "    print(\"-\"*45)\n",
    "    print(f\"Reading table for page {page} and entity {entity}.\")\n",
    "    \n",
    "    # first table needs to be read in with flavor=\"stream\"\n",
    "    # flavor=\"lattice\" raises an error, maybe camelot issue\n",
    "    # see https://github.com/atlanhq/camelot/issues/306\n",
    "    # or because characters in first row almost reach\n",
    "    # the table grid    \n",
    "    if page == '131':\n",
    "        tables_inventory_original = camelot.read_pdf(\n",
    "            str(input_folder / pdf_file),\n",
    "            pages=page,\n",
    "            table_areas=page_def_templates[page][\"area\"],\n",
    "            columns=page_def_templates[page][\"cols\"],\n",
    "            flavor=\"stream\",\n",
    "            split_text=True\n",
    "        )\n",
    "        \n",
    "        df_trend_entity = tables_inventory_original[0].df[1:]\n",
    "    else:\n",
    "        tables_inventory_original = camelot.read_pdf(\n",
    "            str(input_folder / pdf_file),\n",
    "            pages=page,\n",
    "            flavor=\"lattice\",\n",
    "            split_text=True)\n",
    "        df_trend_entity = tables_inventory_original[0].df[3:]\n",
    "\n",
    "    print(f\"Reading complete.\")\n",
    "\n",
    "    # add columns\n",
    "    # 'data' prefix is needed for pd.wide_to_long() later\n",
    "    columns_years = ['data1990', 'data1995', \"data2000\", 'data2005', 'data2010', 'data2015', 'data2018', 'data2019']\n",
    "    df_trend_entity.columns = ['orig_cat_name'] + columns_years\n",
    "    \n",
    "    # unit is always Gg\n",
    "    df_trend_entity['unit'] = 'Gg'\n",
    "    \n",
    "    # only one entity per table\n",
    "    df_trend_entity['entity'] = entity\n",
    "    \n",
    "    df_trend_entity[\"category\"] = df_trend_entity[\"orig_cat_name\"]\n",
    "\n",
    "    # delete rows that are just a headline or empty\n",
    "    #row_to_delete = df_trend_entity.index[df_trend_entity['category'] == 'Éléments pour mémoire'][0]\n",
    "    #df_trend_entity = df_trend_entity.drop(index = row_to_delete)\n",
    "\n",
    "    # in the first table there is no empty line\n",
    "    if page != '131':\n",
    "        row_to_delete = df_trend_entity.index[df_trend_entity['category'] == ''][0]\n",
    "        df_trend_entity = df_trend_entity.drop(index = row_to_delete)\n",
    "        \n",
    "    inv_conf[\"cat_code_regexp\"] = r'^(?P<code>[a-zA-Z0-9\\.]{1,11})[\\s\\.].*'\n",
    "\n",
    "    df_trend_entity[\"category\"] = df_trend_entity[\"category\"].replace(\n",
    "        {\n",
    "         'Total des émissions et absorptions nationales': \"0\",\n",
    "         '2A5: Autre' : '2A5',\n",
    "         'Éléments pour mémoire': 'MEMO',\n",
    "         'Soutes internationales' : 'M.BK',\n",
    "         '1.A.3.a.i - Aviation internationale (soutes internationales)' : 'M.BK.A',\n",
    "         '1.A.3.d.i - Navigation internationale (soutes internationales)' : 'M.BK.M',\n",
    "         '1.A.5.c - Opérations multilatérales' : 'M.MULTIOP',\n",
    "        })\n",
    "\n",
    "    df_trend_entity[\"category\"] = df_trend_entity[\"category\"].str.replace(\".\", \"\")\n",
    "    df_trend_entity[\"category\"] = df_trend_entity[\"category\"].str.replace(\"\\n\", \"\")\n",
    "    \n",
    "    \n",
    "    repl = lambda m: m.group('code')\n",
    "    df_trend_entity[\"category\"] = \\\n",
    "        df_trend_entity[\"category\"].str.replace(inv_conf[\"cat_code_regexp\"], repl,\n",
    "                                              regex=True)\n",
    "    \n",
    "    df_trend_entity = df_trend_entity.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Created category codes.\")\n",
    "    \n",
    "    for year in columns_years:\n",
    "        df_trend_entity[year] = df_trend_entity[year].str.replace(\",\", \".\")\n",
    "        df_trend_entity[year] = df_trend_entity[year].str.replace(\"NE1\", \"NE\")\n",
    "    \n",
    "    # make sure all col headers are str\n",
    "    df_trend_entity.columns = df_trend_entity.columns.map(str)\n",
    "    \n",
    "    df_trend_entity = df_trend_entity.drop(columns=[\"orig_cat_name\"])\n",
    "\n",
    "    # TODO wide in IF gibt es convert_wide_dataframe_if\n",
    "    df_trend_entity_long = pd.wide_to_long(df_trend_entity, stubnames='data',  i='category', j='time')\n",
    "    \n",
    "    print(f\"Converted to long format.\")\n",
    "    \n",
    "    df_trend_entity_long = df_trend_entity_long.reset_index()\n",
    "    \n",
    "    df_main_dict[page] =  df_trend_entity_long\n",
    "\n",
    "print(\"Converting to interchange format.\")\n",
    "\n",
    "df_trend_all = pd.concat([df_main_dict['131'], df_main_dict['132']], axis=0, join='outer').reset_index(drop=True)\n",
    "\n",
    "df_trend_IF = pm2.pm2io.convert_long_dataframe_if(\n",
    "    df_trend_all,\n",
    "    coords_cols=coords_cols,\n",
    "    #add_coords_cols=add_coords_cols,\n",
    "    coords_defaults=coords_defaults,\n",
    "    coords_terminologies=coords_terminologies,\n",
    "    coords_value_mapping=coords_value_mapping['trend'],\n",
    "    #coords_value_filling=coords_value_filling,\n",
    "    filter_remove=filter_remove,\n",
    "    #filter_keep=filter_keep,\n",
    "    meta_data=meta_data,\n",
    "    convert_str=True,\n",
    "    time_format=\"%Y\",\n",
    "    )\n",
    "    \n",
    "df_trend_IF\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e1ad4f-c35c-460c-8546-5e493f363739",
   "metadata": {},
   "outputs": [],
   "source": [
    "### convert to primap2 format ###\n",
    "data_pm2_trend = pm2.pm2io.from_interchange_format(df_trend_IF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b65227-b7c4-4d18-89ef-af927c9a81b5",
   "metadata": {},
   "source": [
    "# Combine tables and save to IF and native format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960117b6-28fc-45ba-a768-16f63e428875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### combine\n",
    "\n",
    "#data_pm2_main\n",
    "#data_pm2_trend\n",
    "#data_pm2_energy\n",
    "#data_pm2_lulucf\n",
    "#data_pm2_waste\n",
    "\n",
    "# tolerance needs to be high as rounding in trend tables leads to inconsistent data\n",
    "# increase tolerance \n",
    "# check if value from another year (not now)\n",
    "# if error to high delete the category (for not so important entities)\n",
    "data_pm2 = data_pm2_main.pr.merge(data_pm2_energy,tolerance=0.11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb74c9e-b400-454b-848a-28091b832016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert back to IF to have units in the fixed format\n",
    "data_if = data_pm2.pr.to_interchange_format()\n",
    "\n",
    "# ###\n",
    "# save data to IF and native format\n",
    "# ###\n",
    "pm2.pm2io.write_interchange_format(\n",
    "    output_folder / (output_filename + coords_terminologies[\"category\"] + \"_raw\"), data_if)\n",
    "\n",
    "encoding = {var: compression for var in data_pm2.data_vars}\n",
    "data_pm2.pr.to_netcdf(\n",
    "    output_folder / (output_filename + coords_terminologies[\"category\"] + \"_raw.nc\"),\n",
    "    encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30257c41-9c0d-44c1-bda9-4b0c29a1ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing for removals not neccessary here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc34733a-919b-4d11-a300-f4a3695a0a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_proc_pm2 = process_data_for_country(\n",
    "    data_proc_pm2,\n",
    "    entities_to_ignore=[],\n",
    "    gas_baskets=gas_baskets,\n",
    "    processing_info_country=country_processing_step2, # maybe step 2 not necessary\n",
    "    cat_terminology_out = terminology_proc,\n",
    "    category_conversion = cat_conversion, # probably not\n",
    "    sectors_out = sectors_to_save,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
